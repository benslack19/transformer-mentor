{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650940f2",
   "metadata": {},
   "source": [
    "# Transforms\n",
    "\n",
    "This is taking the data and putting it into the processed forms for the models (e.g. text to numbers or more speciically tensors). \n",
    "\n",
    "- `transform`: modify the features (x)\n",
    "- `target_transform`: modify the labels (y)\n",
    "\n",
    "The [`torchvision.transforms`](https://docs.pytorch.org/vision/stable/transforms.html) module offers several commonly-used transforms out of the box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d56863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Lambda, ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad8ef296",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8971e40",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "The FashionMNIST features are in PIL Image format, and the labels are integers. We want:\n",
    "- features: PIL Image format > normalized tensors\n",
    "- labels: integers > one-hot encoded tensors\n",
    "\n",
    "Normalized tensor: Values have been scaled, typically to 0 and 1 (i.e. not a unit vector but provides some similar advantages practically). This supports:\n",
    "- training stability: inputs are in smaller, more consistent ranges\n",
    "- gradient flow: helps prevent vanishing/exploding gradients (gradient can shrink or explode across layers, leading to later layers getting big/small updates, respectively)\n",
    "- optimization efficiency: loss landscape is smoother for optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ade96f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb9e5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=\"../local_data/transformer-mentor_data\"\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=DATA_DIR,\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=ToTensor(),\n",
    "    # target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395f99f",
   "metadata": {},
   "source": [
    "Let's focus on the above uses of `ToTensor()` and `Lambda`.\n",
    "\n",
    "# `ToTensor()`\n",
    "\n",
    "Converts a PIL image or NumPy `ndarray` to a `FloatTensor` and scales the image's pixel intensity values in the range `[0., 1.]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dace6e2",
   "metadata": {},
   "source": [
    "# Lambda Transforms\n",
    "\n",
    "Allows for user-defined lambda functions. In this example, here's a way to turn the integers into a one-hot-encoded tensor in the following way:\n",
    "- create a zero tensor of size 10 (number of labels)\n",
    "- call `scatter_1` which assigns a `value=1` on the index as given by the label `y`\n",
    "\n",
    "This was already applied in the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89650081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0,  ..., 3, 0, 5])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f625dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = ds.targets[0:10]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5631852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(10, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f45376a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96706/845930082.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y[0]), value=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y[0]), value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1777747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2025-08-16T16:10:40.267800+00:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.12.11\n",
      "IPython version      : 9.4.0\n",
      "\n",
      "Compiler    : GCC 12.2.0\n",
      "OS          : Linux\n",
      "Release     : 6.10.14-linuxkit\n",
      "Machine     : aarch64\n",
      "Processor   : \n",
      "CPU cores   : 7\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "630df768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchvision: 0.22.1\n",
      "torch      : 2.7.1\n",
      "numpy      : 2.3.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -iv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
